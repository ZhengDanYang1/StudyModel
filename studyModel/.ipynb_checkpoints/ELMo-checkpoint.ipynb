{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharCNNEmbedding:\n",
    "    def __init__(self, config):\n",
    "        self.char_vocab_size = config[\"char_vocab_size\"]\n",
    "        self.char_embedding_dim = config[\"char_embedding_dim\"]\n",
    "\n",
    "        self.kernel_sizes = config[\"kernel_sizes\"]\n",
    "        self.filter_size = config[\"elmo_hidden\"] // len(self.kernel_sizes)\n",
    "\n",
    "        self.seq_len = config[\"word_seq_len\"]\n",
    "        self.char_seq_len = config[\"char_seq_len\"]\n",
    "        \n",
    "        with tf.variable_scope(\"char_cnn\", reuse=tf.AUTO_REUSE):\n",
    "            self.conv_filters = [\n",
    "                tf.layers.Conv1D(self.filter_size, kernel_size)\n",
    "                for kernel_size in self.kernel_sizes\n",
    "            ]\n",
    "\n",
    "        with tf.variable_scope(\"char_embedding\", reuse=tf.AUTO_REUSE):\n",
    "            self.embedding_weight = tf.get_variable(\"embedding_weight\", \n",
    "                                        [self.char_vocab_size, self.char_embedding_dim],\n",
    "                                        dtype=tf.float32)\n",
    "            \n",
    "            \n",
    "    def forward(self, data):\n",
    "        embed_input = tf.nn.embedding_lookup(self.embedding_weight, data[\"input\"])\n",
    "\n",
    "        conv_outputs = []\n",
    "        conv_input = tf.reshape(embed_input, [-1, self.char_seq_len, self.char_embedding_dim])\n",
    "        for conv, kernel_size in zip(self.conv_filters, self.kernel_sizes):\n",
    "            conv_output = conv(conv_input)\n",
    "            _conv_output = tf.reshape(conv_output, [-1, self.seq_len, conv_output.shape[1], self.filter_size])\n",
    "\n",
    "            pool_output = tf.nn.max_pool(_conv_output, [1, 1, conv_output.shape[1], 1], [1, 1, 1, 1], 'VALID')\n",
    "            pool_output = tf.squeeze(pool_output, axis=2)\n",
    "            conv_outputs.append(pool_output)\n",
    "\n",
    "        # shape = (batch_size, seq_len, embedding_dim)\n",
    "        char_word_embedding = tf.concat(conv_outputs, axis=2)\n",
    "        return char_word_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELMO:\n",
    "    def __init__(self, config):\n",
    "        self.embedding = CharCNNEmbedding(config)\n",
    "        self.hidden_size = config[\"elmo_hidden\"]\n",
    "        self.vocab_size = config[\"word_vocab_size\"]\n",
    "        self.seq_len = config[\"word_seq_len\"]\n",
    "        self.config = config\n",
    "        \n",
    "        with tf.variable_scope(\"elmo_rnn_cell\"):\n",
    "            self.forward_cell = tf.nn.rnn_cell.LSTMCell(self.hidden_size, reuse=tf.AUTO_REUSE)\n",
    "            self.backward_cell = tf.nn.rnn_cell.LSTMCell(self.hidden_size, reuse=tf.AUTO_REUSE)\n",
    "        \n",
    "        #是否将输入concat到输出\n",
    "        if config.get(\"use_skip_connection\"):\n",
    "            self.forward_cell = tf.nn.rnn_cell.ResidualWrapper(self.forward_cell)\n",
    "            self.backward_cell = tf.nn.rnn_cell.ResidualWrapper(self.backward_cell)\n",
    "            \n",
    "        with tf.variable_scope(\"elmo_softmax\"):\n",
    "            softmax_weight_shape = [config[\"word_vocab_size\"], config[\"elmo_hidden\"]]\n",
    "\n",
    "            self.forward_softmax_w = tf.get_variable(\"forward_softmax_w\", softmax_weight_shape, dtype=tf.float32)\n",
    "            self.backward_softmax_w = tf.get_variable(\"backward_softmax_w\", softmax_weight_shape, dtype=tf.float32)\n",
    "\n",
    "            self.forward_softmax_b = tf.get_variable(\"forward_softmax_b\", [config[\"word_vocab_size\"]])\n",
    "            self.backward_softmax_b = tf.get_variable(\"backward_softmax_b\", [config[\"word_vocab_size\"]])\n",
    "            \n",
    "    def forward(self, data):\n",
    "        embedding_output = self.embedding.forward(data)\n",
    "        with tf.variable_scope(\"elmo_rnn_forward\"):\n",
    "            forward_outputs, forward_states = tf.nn.dynamic_rnn(self.forward_cell,\n",
    "                                                                inputs=embedding_output,\n",
    "                                                                sequence_length=data[\"input_len\"],\n",
    "                                                                dtype=tf.float32)\n",
    "\n",
    "        with tf.variable_scope(\"elmo_rnn_backward\"):\n",
    "            backward_outputs, backward_states = tf.nn.dynamic_rnn(self.backward_cell,\n",
    "                                                                  inputs=embedding_output,\n",
    "                                                                  sequence_length=data[\"input_len\"],\n",
    "                                                                  dtype=tf.float32)\n",
    "\n",
    "        # # Concatenate the forward and backward LSTM output\n",
    "        forward_projection = tf.matmul(forward_outputs, tf.expand_dims(tf.transpose(self.forward_softmax_w), 0))\n",
    "        forward_projection = tf.nn.bias_add(forward_projection, self.forward_softmax_b)\n",
    "\n",
    "        backward_projection = tf.matmul(backward_outputs, tf.expand_dims(tf.transpose(self.backward_softmax_w), 0))\n",
    "        backward_projection = tf.nn.bias_add(backward_projection, self.backward_softmax_b)\n",
    "\n",
    "        return forward_outputs, backward_outputs, forward_projection, backward_projection\n",
    "    \n",
    "    \n",
    "    def train(self, data, global_step_variable=None):\n",
    "        forward_output, backward_output, _, _ = self.forward(data)\n",
    "\n",
    "        forward_target = data[\"target\"]\n",
    "        forward_pred = tf.cast(tf.argmax(tf.nn.softmax(forward_output, -1), -1), tf.int32)\n",
    "        forward_correct = tf.equal(forward_pred, forward_target)\n",
    "        forward_padding = tf.sequence_mask(data[\"target_len\"], maxlen=self.seq_len, dtype=tf.float32)\n",
    "\n",
    "        forward_softmax_target = tf.cast(tf.reshape(forward_target, [-1, 1]), tf.int64)\n",
    "        forward_softmax_input = tf.reshape(forward_output, [-1, self.hidden_size])\n",
    "        forward_train_loss = tf.nn.sampled_softmax_loss(\n",
    "            weights=self.forward_softmax_w, biases=self.forward_softmax_b,\n",
    "            labels=forward_softmax_target, inputs=forward_softmax_input,\n",
    "            num_sampled=self.config[\"softmax_sample_size\"],\n",
    "            num_classes=self.config[\"word_vocab_size\"]\n",
    "        )\n",
    "\n",
    "        forward_train_loss = tf.reshape(forward_train_loss, [-1, self.seq_len])\n",
    "        forward_train_loss = tf.multiply(forward_train_loss, forward_padding)\n",
    "        forward_train_loss = tf.reduce_mean(forward_train_loss)\n",
    "\n",
    "        backward_target = tf.reverse_sequence(data[\"target\"], data[\"target_len\"], seq_axis=1, batch_axis=0)\n",
    "        backward_pred = tf.cast(tf.argmax(tf.nn.softmax(backward_output, -1), -1), tf.int32)\n",
    "        backward_correct = tf.equal(backward_pred, backward_target)\n",
    "        backward_padding = tf.sequence_mask(data[\"target_len\"], maxlen=self.seq_len, dtype=tf.float32)\n",
    "\n",
    "        backward_softmax_target = tf.cast(tf.reshape(backward_target, [-1, 1]), tf.int64)\n",
    "        backward_softmax_input = tf.reshape(backward_output, [-1, self.hidden_size])\n",
    "        backward_train_loss = tf.nn.sampled_softmax_loss(\n",
    "            weights=self.backward_softmax_w, biases=self.backward_softmax_b,\n",
    "            labels=backward_softmax_target, inputs=backward_softmax_input,\n",
    "            num_sampled=self.config[\"softmax_sample_size\"],\n",
    "            num_classes=self.config[\"word_vocab_size\"]\n",
    "        )\n",
    "\n",
    "        backward_train_loss = tf.reshape(backward_train_loss, [-1, self.seq_len])\n",
    "        backward_train_loss = tf.multiply(backward_train_loss, backward_padding)\n",
    "        backward_train_loss = tf.reduce_mean(backward_train_loss)\n",
    "\n",
    "        train_loss = forward_train_loss + backward_train_loss\n",
    "        train_correct = tf.concat([forward_correct, backward_correct], axis=-1)\n",
    "        train_acc = tf.reduce_mean(tf.cast(train_correct, tf.float32))\n",
    "\n",
    "        tf.summary.scalar(\"train_acc\", train_acc)\n",
    "        tf.summary.scalar(\"train_loss\", train_loss)\n",
    "\n",
    "        train_ops = tf.train.AdamOptimizer().minimize(train_loss)\n",
    "        return train_loss, train_acc, train_ops\n",
    "\n",
    "    def pred(self, data):\n",
    "        elmo_projection_output = self.forward(data)\n",
    "        eval_output = tf.nn.softmax(elmo_projection_output, dim=-1)\n",
    "        return eval_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 1024, 'corpus_files': ['data/corpus/elmo.corpus.xlarge.1.txt'], 'epochs': 10, 'verbose_freq': 1, 'word_vocab_path': 'data/vocab/word.90k.vocab', 'char_vocab_path': 'data/vocab/jamo.100.vocab', 'word_seq_len': 10, 'char_seq_len': 7, 'char_embedding_dim': 64, 'kernel_sizes': [1, 2, 3, 4], 'filter_sizes': None, 'elmo_hidden': 512, 'softmax_sample_size': 8196, 'prefetch_size': 1024, 'log_dir': 'logs/', 'save_freq': 1000, 'model_save_path': 'output/elmo.model.test', 'log_file_prefix': 'elmo.log'}\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-b\", \"--batch_size\", type=int, default=1024)\n",
    "parser.add_argument(\"-c\", \"--corpus_files\", nargs='+', type=str,\n",
    "                    default=[\"data/corpus/elmo.corpus.xlarge.1.txt\"])\n",
    "\n",
    "parser.add_argument(\"-e\", \"--epochs\", type=int, default=10)\n",
    "parser.add_argument(\"--verbose_freq\", type=int, default=1)\n",
    "\n",
    "parser.add_argument(\"--word_vocab_path\", type=str, default=\"data/vocab/word.90k.vocab\")\n",
    "parser.add_argument(\"--char_vocab_path\", type=str, default=\"data/vocab/jamo.100.vocab\")\n",
    "\n",
    "parser.add_argument(\"--word_seq_len\", type=int, default=10)\n",
    "parser.add_argument(\"--char_seq_len\", type=int, default=7)\n",
    "\n",
    "parser.add_argument(\"--char_embedding_dim\", type=int, default=64)\n",
    "parser.add_argument(\"--kernel_sizes\", nargs='+', type=int, default=[1, 2, 3, 4])\n",
    "parser.add_argument(\"--filter_sizes\", nargs='+', type=int, default=None)\n",
    "\n",
    "parser.add_argument(\"--elmo_hidden\", type=int, default=512)\n",
    "parser.add_argument(\"--softmax_sample_size\", type=int, default=8196)\n",
    "\n",
    "parser.add_argument(\"--prefetch_size\", type=int, default=1024)\n",
    "\n",
    "parser.add_argument(\"--log_dir\", type=str, default=\"logs/\")\n",
    "parser.add_argument(\"--save_freq\", type=int, default=1000)\n",
    "parser.add_argument(\"--model_save_path\", type=str, default=\"output/elmo.model.test\")\n",
    "parser.add_argument(\"--log_file_prefix\", type=str, default=\"elmo.log\")\n",
    "args = parser.parse_known_args()[0]\n",
    "config_dict = vars(args)\n",
    "\n",
    "print(config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from han2jamo import Han2Jamo\n",
    "from vocab_builder import CharWordVocab, WordVocab\n",
    "\n",
    "\n",
    "class ElmoKoreanDataset:\n",
    "    def __init__(self, config):\n",
    "        self.corpus_files = config[\"corpus_files\"]\n",
    "        self.jamo_processor = Han2Jamo()\n",
    "\n",
    "        self.char_vocab = CharWordVocab.load_vocab(config[\"char_vocab_path\"])\n",
    "        self.word_vocab = WordVocab.load_vocab(config[\"word_vocab_path\"])\n",
    "\n",
    "        self.seq_len = config[\"word_seq_len\"]\n",
    "        self.char_seq_len = config[\"char_seq_len\"]\n",
    "        self.corpus_size = self.get_corpus_size()\n",
    "        print(\"Dataset Size:\", self.corpus_size)\n",
    "\n",
    "        config[\"char_vocab_size\"] = len(self.char_vocab)\n",
    "        config[\"word_vocab_size\"] = len(self.word_vocab)\n",
    "\n",
    "    def text_to_char_sequence(self, text):\n",
    "        jamo_text = self.jamo_processor.str_to_jamo(text)\n",
    "        char_idx_seq, seq_len = self.char_vocab.to_seq(jamo_text,\n",
    "                                                       char_seq_len=self.char_seq_len,\n",
    "                                                       seq_len=self.seq_len,\n",
    "                                                       with_len=True)\n",
    "        seq_len = self.seq_len if seq_len > self.seq_len else seq_len\n",
    "        return char_idx_seq, seq_len\n",
    "\n",
    "    def text_to_word_sequence(self, text):\n",
    "        word_idx_seq, seq_len = self.word_vocab.to_seq(text, seq_len=self.seq_len + 1, with_len=True, with_eos=True)\n",
    "        seq_len = self.seq_len + 1 if seq_len > self.seq_len + 1 else seq_len\n",
    "        word_idx_seq, seq_len = word_idx_seq[1:], seq_len - 1\n",
    "        return word_idx_seq, seq_len\n",
    "\n",
    "    def produce_data(self, text):\n",
    "        text = text.strip()\n",
    "        char_word_input, input_len = self.text_to_char_sequence(text)\n",
    "        word_target, target_len = self.text_to_word_sequence(text)\n",
    "\n",
    "        return {\"input\": char_word_input, \"input_len\": input_len,\n",
    "                \"target\": word_target, \"target_len\": target_len}\n",
    "\n",
    "    def data_generator(self):\n",
    "        for file_path in self.corpus_files:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                for text in f:\n",
    "                    yield self.produce_data(text)\n",
    "\n",
    "    def get_corpus_size(self):\n",
    "        count = 0\n",
    "        for file_path in self.corpus_files:\n",
    "            with open(file_path) as file:\n",
    "                count += sum(1 for _ in file)\n",
    "        return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CharCNNEmbedding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-3164dfa731ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0melmo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mELMO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-0db0d3113394>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mELMO\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCharCNNEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"elmo_hidden\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"word_vocab_size\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CharCNNEmbedding' is not defined"
     ]
    }
   ],
   "source": [
    "elmo = ELMO(config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
