{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from data_load import load_vocab\n",
    "from modules import get_token_embeddings, ff, positional_encoding, multihead_attention, label_smoothing, noam_scheme\n",
    "from utils import convert_idx_to_token_tensor\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer:\n",
    "    '''\n",
    "    xs: tuple of\n",
    "        x: int32 tensor. (N, T1)\n",
    "        x_seqlens: int32 tensor. (N,)\n",
    "        sents1: str tensor. (N,)\n",
    "    ys: tuple of\n",
    "        decoder_input: int32 tensor. (N, T2)\n",
    "        y: int32 tensor. (N, T2)\n",
    "        y_seqlen: int32 tensor. (N, )\n",
    "        sents2: str tensor. (N,)\n",
    "    training: boolean.\n",
    "    '''\n",
    "    def __init__(self, hp):\n",
    "        self.hp = hp\n",
    "        self.token2idx, self.idx2token = load_vocab(hp.vocab)\n",
    "        self.embeddings = get_token_embeddings(self.hp.vocab_size, self.hp.d_model, zero_pad=True)\n",
    "\n",
    "    def encode(self, xs, training=True):\n",
    "        '''\n",
    "        Returns\n",
    "        memory: encoder outputs. (N, T1, d_model)\n",
    "        '''\n",
    "        with tf.variable_scope(\"encoder\", reuse=tf.AUTO_REUSE):\n",
    "            x, seqlens, sents1 = xs\n",
    "\n",
    "            # src_masks\n",
    "            src_masks = tf.math.equal(x, 0) # (N, T1)\n",
    "\n",
    "            # embedding\n",
    "            enc = tf.nn.embedding_lookup(self.embeddings, x) # (N, T1, d_model)\n",
    "            enc *= self.hp.d_model**0.5 # scale\n",
    "\n",
    "            enc += positional_encoding(enc, self.hp.maxlen1)\n",
    "            enc = tf.layers.dropout(enc, self.hp.dropout_rate, training=training)\n",
    "\n",
    "            ## Blocks\n",
    "            for i in range(self.hp.num_blocks):\n",
    "                with tf.variable_scope(\"num_blocks_{}\".format(i), reuse=tf.AUTO_REUSE):\n",
    "                    # self-attention\n",
    "                    enc = multihead_attention(queries=enc,\n",
    "                                              keys=enc,\n",
    "                                              values=enc,\n",
    "                                              key_masks=src_masks,\n",
    "                                              num_heads=self.hp.num_heads,\n",
    "                                              dropout_rate=self.hp.dropout_rate,\n",
    "                                              training=training,\n",
    "                                              causality=False)\n",
    "                    # feed forward\n",
    "                    enc = ff(enc, num_units=[self.hp.d_ff, self.hp.d_model])\n",
    "        memory = enc\n",
    "        return memory, sents1, src_masks\n",
    "\n",
    "    def decode(self, ys, memory, src_masks, training=True):\n",
    "        '''\n",
    "        memory: encoder outputs. (N, T1, d_model)\n",
    "        src_masks: (N, T1)\n",
    "\n",
    "        Returns\n",
    "        logits: (N, T2, V). float32.\n",
    "        y_hat: (N, T2). int32\n",
    "        y: (N, T2). int32\n",
    "        sents2: (N,). string.\n",
    "        '''\n",
    "        with tf.variable_scope(\"decoder\", reuse=tf.AUTO_REUSE):\n",
    "            decoder_inputs, y, seqlens, sents2 = ys\n",
    "\n",
    "            # tgt_masks\n",
    "            tgt_masks = tf.math.equal(decoder_inputs, 0)  # (N, T2)\n",
    "\n",
    "            # embedding\n",
    "            dec = tf.nn.embedding_lookup(self.embeddings, decoder_inputs)  # (N, T2, d_model)\n",
    "            dec *= self.hp.d_model ** 0.5  # scale\n",
    "\n",
    "            dec += positional_encoding(dec, self.hp.maxlen2)\n",
    "            dec = tf.layers.dropout(dec, self.hp.dropout_rate, training=training)\n",
    "\n",
    "            # Blocks\n",
    "            for i in range(self.hp.num_blocks):\n",
    "                with tf.variable_scope(\"num_blocks_{}\".format(i), reuse=tf.AUTO_REUSE):\n",
    "                    # Masked self-attention (Note that causality is True at this time)\n",
    "                    dec = multihead_attention(queries=dec,\n",
    "                                              keys=dec,\n",
    "                                              values=dec,\n",
    "                                              key_masks=tgt_masks,\n",
    "                                              num_heads=self.hp.num_heads,\n",
    "                                              dropout_rate=self.hp.dropout_rate,\n",
    "                                              training=training,\n",
    "                                              causality=True,\n",
    "                                              scope=\"self_attention\")\n",
    "\n",
    "                    # Vanilla attention\n",
    "                    dec = multihead_attention(queries=dec,\n",
    "                                              keys=memory,\n",
    "                                              values=memory,\n",
    "                                              key_masks=src_masks,\n",
    "                                              num_heads=self.hp.num_heads,\n",
    "                                              dropout_rate=self.hp.dropout_rate,\n",
    "                                              training=training,\n",
    "                                              causality=False,\n",
    "                                              scope=\"vanilla_attention\")\n",
    "                    ### Feed Forward\n",
    "                    dec = ff(dec, num_units=[self.hp.d_ff, self.hp.d_model])\n",
    "\n",
    "        # Final linear projection (embedding weights are shared)\n",
    "        weights = tf.transpose(self.embeddings) # (d_model, vocab_size)\n",
    "        logits = tf.einsum('ntd,dk->ntk', dec, weights) # (N, T2, vocab_size)\n",
    "        y_hat = tf.to_int32(tf.argmax(logits, axis=-1))\n",
    "\n",
    "        return logits, y_hat, y, sents2\n",
    "\n",
    "    def train(self, xs, ys):\n",
    "        '''\n",
    "        Returns\n",
    "        loss: scalar.\n",
    "        train_op: training operation\n",
    "        global_step: scalar.\n",
    "        summaries: training summary node\n",
    "        '''\n",
    "        # forward\n",
    "        memory, sents1, src_masks = self.encode(xs)\n",
    "        logits, preds, y, sents2 = self.decode(ys, memory, src_masks)\n",
    "\n",
    "        # train scheme\n",
    "        y_ = label_smoothing(tf.one_hot(y, depth=self.hp.vocab_size))\n",
    "        ce = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y_)\n",
    "        nonpadding = tf.to_float(tf.not_equal(y, self.token2idx[\"<pad>\"]))  # 0: <pad>\n",
    "        loss = tf.reduce_sum(ce * nonpadding) / (tf.reduce_sum(nonpadding) + 1e-7)\n",
    "\n",
    "        global_step = tf.train.get_or_create_global_step()\n",
    "        lr = noam_scheme(self.hp.lr, global_step, self.hp.warmup_steps)\n",
    "        optimizer = tf.train.AdamOptimizer(lr)\n",
    "        train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "        tf.summary.scalar('lr', lr)\n",
    "        tf.summary.scalar(\"loss\", loss)\n",
    "        tf.summary.scalar(\"global_step\", global_step)\n",
    "\n",
    "        summaries = tf.summary.merge_all()\n",
    "\n",
    "        return loss, train_op, global_step, summaries\n",
    "\n",
    "    def eval(self, xs, ys):\n",
    "        '''Predicts autoregressively\n",
    "        At inference, input ys is ignored.\n",
    "        Returns\n",
    "        y_hat: (N, T2)\n",
    "        '''\n",
    "        decoder_inputs, y, y_seqlen, sents2 = ys\n",
    "\n",
    "        decoder_inputs = tf.ones((tf.shape(xs[0])[0], 1), tf.int32) * self.token2idx[\"<s>\"]\n",
    "        ys = (decoder_inputs, y, y_seqlen, sents2)\n",
    "\n",
    "        memory, sents1, src_masks = self.encode(xs, False)\n",
    "\n",
    "        logging.info(\"Inference graph is being built. Please be patient.\")\n",
    "        for _ in tqdm(range(self.hp.maxlen2)):\n",
    "            logits, y_hat, y, sents2 = self.decode(ys, memory, src_masks, False)\n",
    "            if tf.reduce_sum(y_hat, 1) == self.token2idx[\"<pad>\"]: break\n",
    "\n",
    "            _decoder_inputs = tf.concat((decoder_inputs, y_hat), 1)\n",
    "            ys = (_decoder_inputs, y, y_seqlen, sents2)\n",
    "\n",
    "        # monitor a random sample\n",
    "        n = tf.random_uniform((), 0, tf.shape(y_hat)[0]-1, tf.int32)\n",
    "        sent1 = sents1[n]\n",
    "        pred = convert_idx_to_token_tensor(y_hat[n], self.idx2token)\n",
    "        sent2 = sents2[n]\n",
    "\n",
    "        tf.summary.text(\"sent1\", sent1)\n",
    "        tf.summary.text(\"pred\", pred)\n",
    "        tf.summary.text(\"sent2\", sent2)\n",
    "        summaries = tf.summary.merge_all()\n",
    "\n",
    "        return y_hat, summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_load import get_batch\n",
    "from utils import save_hparams, save_variable_specs, get_hypotheses, calc_bleu\n",
    "import os\n",
    "from hparams import Hparams\n",
    "import math\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging.info(\"# hparams\")\n",
    "hparams = Hparams()\n",
    "parser = hparams.parser\n",
    "hp = parser.parse_known_args()[0]\n",
    "save_hparams(hp, hp.logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(fpath1, fpath2, maxlen1, maxlen2, vocab_fpath, batch_size, shuffle=False):\n",
    "    '''Gets training / evaluation mini-batches\n",
    "    fpath1: source file path. string.\n",
    "    fpath2: target file path. string.\n",
    "    maxlen1: source sent maximum length. scalar.\n",
    "    maxlen2: target sent maximum length. scalar.\n",
    "    vocab_fpath: string. vocabulary file path.\n",
    "    batch_size: scalar\n",
    "    shuffle: boolean\n",
    "\n",
    "    Returns\n",
    "    batches\n",
    "    num_batches: number of mini-batches\n",
    "    num_samples\n",
    "    '''\n",
    "    sents1, sents2 = load_data(fpath1, fpath2, maxlen1, maxlen2)\n",
    "    batches = input_fn(sents1, sents2, vocab_fpath, batch_size, shuffle=shuffle)\n",
    "    num_batches = calc_num_batches(len(sents1), batch_size)\n",
    "    return batches, num_batches, len(sents1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(fpath1, fpath2, maxlen1, maxlen2):\n",
    "    '''Loads source and target data and filters out too lengthy samples.\n",
    "    fpath1: source file path. string.\n",
    "    fpath2: target file path. string.\n",
    "    maxlen1: source sent maximum length. scalar.\n",
    "    maxlen2: target sent maximum length. scalar.\n",
    "\n",
    "    Returns\n",
    "    sents1: list of source sents\n",
    "    sents2: list of target sents\n",
    "    '''\n",
    "    sents1, sents2 = [], []\n",
    "    with open(fpath1, 'r') as f1, open(fpath2, 'r') as f2:\n",
    "        for sent1, sent2 in zip(f1, f2):\n",
    "            if len(sent1.split()) + 1 > maxlen1: continue # 1: </s>\n",
    "            if len(sent2.split()) + 1 > maxlen2: continue  # 1: </s>\n",
    "            sents1.append(sent1.strip())\n",
    "            sents2.append(sent2.strip())\n",
    "    return sents1, sents2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents1, sents2 = load_data('iwslt2016/segmented/train.de.bpe', \n",
    "                           'iwslt2016/segmented/train.en.bpe',\n",
    "                            100, 100,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "dataset = np.array(range(5))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(inp, type, dict):\n",
    "    '''Converts string to number. Used for `generator_fn`.\n",
    "    inp: 1d byte array.\n",
    "    type: \"x\" (source side) or \"y\" (target side)\n",
    "    dict: token2idx dictionary\n",
    "\n",
    "    Returns\n",
    "    list of numbers\n",
    "    '''\n",
    "    inp_str = inp#.decode(\"utf-8\")\n",
    "    if type==\"x\": \n",
    "        tokens = inp_str.split() + [\"</s>\"]#在x的句尾加入结束符\n",
    "        print(tokens)\n",
    "    else: \n",
    "        tokens = [\"<s>\"] + inp_str.split() + [\"</s>\"]#在y的句首句尾加入开始结束符\n",
    "        print(tokens)\n",
    "\n",
    "    x = [dict.get(t, dict[\"<unk>\"]) for t in tokens]\n",
    "    \n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab(vocab_fpath):\n",
    "    '''Loads vocabulary file and returns idx<->token maps\n",
    "    vocab_fpath: string. vocabulary file path.\n",
    "    Note that these are reserved\n",
    "    0: <pad>, 1: <unk>, 2: <s>, 3: </s>\n",
    "\n",
    "    Returns\n",
    "    two dictionaries.\n",
    "    '''\n",
    "    vocab = [line.split()[0] for line in open(vocab_fpath, 'r').read().splitlines()]\n",
    "    token2idx = {token: idx for idx, token in enumerate(vocab)}\n",
    "    idx2token = {idx: token for idx, token in enumerate(vocab)}\n",
    "    return token2idx, idx2token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_fn(sents1, sents2, vocab_fpath):\n",
    "    '''Generates training / evaluation data\n",
    "    sents1: list of source sents\n",
    "    sents2: list of target sents\n",
    "    vocab_fpath: string. vocabulary file path.\n",
    "\n",
    "    yields\n",
    "    xs: tuple of\n",
    "        x: list of source token ids in a sent\n",
    "        x_seqlen: int. sequence length of x\n",
    "        sent1: str. raw source (=input) sentence\n",
    "    labels: tuple of\n",
    "        decoder_input: decoder_input: list of encoded decoder inputs\n",
    "        y: list of target token ids in a sent\n",
    "        y_seqlen: int. sequence length of y\n",
    "        sent2: str. target sentence\n",
    "    '''\n",
    "    token2idx, _ = load_vocab(vocab_fpath)\n",
    "    for sent1, sent2 in zip(sents1, sents2):\n",
    "        x = encode(sent1, \"x\", token2idx)\n",
    "        y = encode(sent2, \"y\", token2idx)\n",
    "        decoder_input, y = y[:-1], y[1:]\n",
    "        x_seqlen, y_seqlen = len(x), len(y)\n",
    "        yield (x, x_seqlen, sent1), (decoder_input, y, y_seqlen, sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent1, sent2 in zip(sents1, sents2):\n",
    "    x = encode(sent1, \"x\", token2idx)\n",
    "    y = encode(sent2, \"y\", token2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = (([None], (), ()),\n",
    "          ([None], [None], (), ()))\n",
    "\n",
    "types = ((tf.int32, tf.int32, tf.string),\n",
    "         (tf.int32, tf.int32, tf.int32, tf.string))\n",
    "\n",
    "\n",
    "paddings = ((0, 0, ''),\n",
    "            (0, 0, 0, ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py:429: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, use\n",
      "    tf.py_function, which takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py:429: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, use\n",
      "    tf.py_function, which takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_generator(\n",
    "    generator_fn,\n",
    "    output_shapes=shapes,\n",
    "    output_types=types,\n",
    "    args=(sents1, sents2, 'iwslt2016/segmented/bpe.vocab'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: (((?,), (), ()), ((?,), (?,), (), ())), types: ((tf.int32, tf.int32, tf.string), (tf.int32, tf.int32, tf.int32, tf.string))>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(sents1, sents2, vocab_fpath, batch_size, shuffle=False):\n",
    "    '''Batchify data\n",
    "    sents1: list of source sents\n",
    "    sents2: list of target sents\n",
    "    vocab_fpath: string. vocabulary file path.\n",
    "    batch_size: scalar\n",
    "    shuffle: boolean\n",
    "\n",
    "    Returns\n",
    "    xs: tuple of\n",
    "        x: int32 tensor. (N, T1)\n",
    "        x_seqlens: int32 tensor. (N,)\n",
    "        sents1: str tensor. (N,)\n",
    "    ys: tuple of\n",
    "        decoder_input: int32 tensor. (N, T2)\n",
    "        y: int32 tensor. (N, T2)\n",
    "        y_seqlen: int32 tensor. (N, )\n",
    "        sents2: str tensor. (N,)\n",
    "    '''\n",
    "    shapes = (([None], (), ()),\n",
    "              ([None], [None], (), ()))\n",
    "    \n",
    "    types = ((tf.int32, tf.int32, tf.string),\n",
    "             (tf.int32, tf.int32, tf.int32, tf.string))\n",
    "    \n",
    "    \n",
    "    paddings = ((0, 0, ''),\n",
    "                (0, 0, 0, ''))\n",
    "\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        generator_fn,\n",
    "        output_shapes=shapes,\n",
    "        output_types=types,\n",
    "        args=(sents1, sents2, vocab_fpath))  # <- arguments for generator_fn. converted to np string arrays\n",
    "\n",
    "    if shuffle: # for training\n",
    "        dataset = dataset.shuffle(128*batch_size)\n",
    "\n",
    "    dataset = dataset.repeat()  # iterate forever\n",
    "    dataset = dataset.padded_batch(batch_size, shapes, paddings).prefetch(1)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = input_fn(sents1, sents2, 'iwslt2016/segmented/bpe.vocab', 128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#logging.info(\"# hparams\")\n",
    "hparams = Hparams()\n",
    "parser = hparams.parser\n",
    "hp = parser.parse_known_args()[0]\n",
    "\n",
    "save_hparams(hp, hp.logdir)\n",
    "\n",
    "#logging.info(\"# Prepare train/eval batches\")\n",
    "train_batches, num_train_batches, num_train_samples = get_batch('iwslt2016/segmented/train.de.bpe', 'iwslt2016/segmented/train.en.bpe',\n",
    "                                             100, 100,\n",
    "                                             'iwslt2016/segmented/bpe.vocab', 128,\n",
    "                                             shuffle=True)\n",
    "eval_batches, num_eval_batches, num_eval_samples = get_batch('iwslt2016/segmented/eval.de.bpe', 'iwslt2016/segmented/eval.en.bpe',\n",
    "                                             100000, 100000,\n",
    "                                             'iwslt2016/segmented/bpe.vocab', 128,\n",
    "                                             shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((TensorShape([Dimension(None), Dimension(None)]),\n",
       "  TensorShape([Dimension(None)]),\n",
       "  TensorShape([Dimension(None)])),\n",
       " (TensorShape([Dimension(None), Dimension(None)]),\n",
       "  TensorShape([Dimension(None), Dimension(None)]),\n",
       "  TensorShape([Dimension(None)]),\n",
       "  TensorShape([Dimension(None)])))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batches.output_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tf.int32, tf.int32, tf.string), (tf.int32, tf.int32, tf.int32, tf.string))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batches.output_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a iterator of the correct shape and type\n",
    "iter = tf.data.Iterator.from_structure(train_batches.output_types, train_batches.output_shapes)\n",
    "xs, ys = iter.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py:358: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py:358: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "train_init_op = iter.make_initializer(train_batches)\n",
    "eval_init_op = iter.make_initializer(eval_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:# Load model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /root/test/transformer-master/modules.py:296: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /root/test/transformer-master/modules.py:296: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-398a9a513a2d>:35: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-398a9a513a2d>:35: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /root/test/transformer-master/modules.py:176: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /root/test/transformer-master/modules.py:176: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-398a9a513a2d>:108: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-398a9a513a2d>:108: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "INFO:root:Inference graph is being built. Please be patient.\n",
      "100%|██████████| 100/100 [02:31<00:00,  1.51s/it]\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"# Load model\")\n",
    "m = Transformer(hp)\n",
    "loss, train_op, global_step, train_summaries = m.train(xs, ys)\n",
    "y_hat, eval_summaries = m.eval(xs, ys)\n",
    "# y_hat = m.infer(xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:# Session\n",
      "INFO:root:Initializing from scratch\n",
      "INFO:root:Variables info has been saved.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_params:  167384067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 1534/30681 [12:00<8:33:54,  1.06s/it]INFO:root:epoch 1 is done\n",
      "INFO:root:# test evaluation\n",
      "INFO:root:# get hypotheses\n",
      "INFO:root:# write results\n",
      "INFO:root:# calc bleu score and append it to translation\n",
      "INFO:root:# save models\n",
      "INFO:root:after training of 1 epochs, log/1/iwslt2016_E01L5.93 has been saved.\n",
      "INFO:root:# fall back to train mode\n",
      " 10%|▉         | 3068/30681 [28:39<3:32:22,  2.17it/s]  INFO:root:epoch 2 is done\n",
      "INFO:root:# test evaluation\n",
      "INFO:root:# get hypotheses\n",
      "INFO:root:# write results\n",
      "INFO:root:# calc bleu score and append it to translation\n",
      "INFO:root:# save models\n",
      "INFO:root:after training of 2 epochs, log/1/iwslt2016_E02L5.20 has been saved.\n",
      "INFO:root:# fall back to train mode\n",
      " 15%|█▍        | 4602/30681 [42:08<3:17:47,  2.20it/s]  INFO:root:epoch 3 is done\n",
      "INFO:root:# test evaluation\n",
      "INFO:root:# get hypotheses\n",
      "INFO:root:# write results\n",
      "INFO:root:# calc bleu score and append it to translation\n",
      "INFO:root:# save models\n",
      "INFO:root:after training of 3 epochs, log/1/iwslt2016_E03L4.47 has been saved.\n",
      "INFO:root:# fall back to train mode\n",
      " 20%|█▉        | 6136/30681 [55:40<3:01:12,  2.26it/s]  INFO:root:epoch 4 is done\n",
      "INFO:root:# test evaluation\n",
      "INFO:root:# get hypotheses\n",
      "INFO:root:# write results\n",
      "INFO:root:# calc bleu score and append it to translation\n",
      "INFO:root:# save models\n",
      "INFO:root:after training of 4 epochs, log/1/iwslt2016_E04L3.97 has been saved.\n",
      "INFO:root:# fall back to train mode\n",
      " 25%|██▍       | 7670/30681 [1:09:08<2:39:24,  2.41it/s]INFO:root:epoch 5 is done\n",
      "INFO:root:# test evaluation\n",
      "INFO:root:# get hypotheses\n",
      "INFO:root:# write results\n",
      "INFO:root:# calc bleu score and append it to translation\n",
      "INFO:root:# save models\n",
      "INFO:root:after training of 5 epochs, log/1/iwslt2016_E05L3.72 has been saved.\n",
      "INFO:root:# fall back to train mode\n",
      " 30%|██▉       | 9204/30681 [1:22:41<3:00:12,  1.99it/s]  INFO:root:epoch 6 is done\n",
      "INFO:root:# test evaluation\n",
      "INFO:root:# get hypotheses\n",
      "INFO:root:# write results\n",
      "INFO:root:# calc bleu score and append it to translation\n",
      "INFO:root:# save models\n",
      "INFO:root:after training of 6 epochs, log/1/iwslt2016_E06L3.49 has been saved.\n",
      "INFO:root:# fall back to train mode\n",
      " 35%|███▍      | 10738/30681 [1:36:09<2:37:54,  2.10it/s] INFO:root:epoch 7 is done\n",
      "INFO:root:# test evaluation\n",
      "INFO:root:# get hypotheses\n",
      "INFO:root:# write results\n",
      "INFO:root:# calc bleu score and append it to translation\n",
      "INFO:root:# save models\n",
      "INFO:root:after training of 7 epochs, log/1/iwslt2016_E07L3.27 has been saved.\n",
      "INFO:root:# fall back to train mode\n",
      " 40%|███▉      | 12272/30681 [1:49:40<2:14:11,  2.29it/s]  INFO:root:epoch 8 is done\n",
      "INFO:root:# test evaluation\n",
      "INFO:root:# get hypotheses\n",
      "INFO:root:# write results\n",
      "INFO:root:# calc bleu score and append it to translation\n",
      "INFO:root:# save models\n",
      "INFO:root:after training of 8 epochs, log/1/iwslt2016_E08L3.21 has been saved.\n",
      "INFO:root:# fall back to train mode\n",
      " 45%|████▍     | 13806/30681 [2:03:18<2:21:55,  1.98it/s]  INFO:root:epoch 9 is done\n",
      "INFO:root:# test evaluation\n",
      "INFO:root:# get hypotheses\n",
      "INFO:root:# write results\n",
      "INFO:root:# calc bleu score and append it to translation\n",
      "INFO:root:# save models\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "log/1/iwslt2016_E09L3.15-13806.data-00000-of-00001.tempstate7212661795582182527; No space left on device\n\t [[node save/SaveV2 (defined at <ipython-input-26-a868abf1b972>:2) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[node save/SaveV2 (defined at <ipython-input-26-a868abf1b972>:2) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'save/SaveV2', defined at:\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 664, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 563, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/platform/asyncio.py\", line 148, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 438, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 1451, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/gen.py\", line 787, in inner\n    self.run()\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 272, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 542, in execute_request\n    user_expressions, allow_stdin,\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2855, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2881, in _run_cell\n    return runner(coro)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 3058, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 3249, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-26-a868abf1b972>\", line 2, in <module>\n    saver = tf.train.Saver(max_to_keep=hp.num_epochs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 832, in __init__\n    self.build()\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 844, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 881, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 510, in _build_internal\n    save_tensor = self._AddSaveOps(filename_tensor, saveables)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 210, in _AddSaveOps\n    save = self.save_op(filename_tensor, saveables)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 124, in save_op\n    tensors)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py\", line 1807, in save_v2\n    name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nResourceExhaustedError (see above for traceback): log/1/iwslt2016_E09L3.15-13806.data-00000-of-00001.tempstate7212661795582182527; No space left on device\n\t [[node save/SaveV2 (defined at <ipython-input-26-a868abf1b972>:2) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[node save/SaveV2 (defined at <ipython-input-26-a868abf1b972>:2) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: log/1/iwslt2016_E09L3.15-13806.data-00000-of-00001.tempstate7212661795582182527; No space left on device\n\t [[{{node save/SaveV2}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[{{node save/SaveV2}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-a868abf1b972>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"# save models\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mckpt_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_gs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"after training of {} epochs, {} has been saved.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs)\u001b[0m\n\u001b[1;32m   1169\u001b[0m           model_checkpoint_path = sess.run(\n\u001b[1;32m   1170\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_tensor_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1171\u001b[0;31m               {self.saver_def.filename_tensor_name: checkpoint_file})\n\u001b[0m\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m         \u001b[0mmodel_checkpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: log/1/iwslt2016_E09L3.15-13806.data-00000-of-00001.tempstate7212661795582182527; No space left on device\n\t [[node save/SaveV2 (defined at <ipython-input-26-a868abf1b972>:2) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[node save/SaveV2 (defined at <ipython-input-26-a868abf1b972>:2) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'save/SaveV2', defined at:\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 664, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 563, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/platform/asyncio.py\", line 148, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 438, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 1451, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/gen.py\", line 787, in inner\n    self.run()\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 272, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 542, in execute_request\n    user_expressions, allow_stdin,\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2855, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2881, in _run_cell\n    return runner(coro)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 3058, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 3249, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-26-a868abf1b972>\", line 2, in <module>\n    saver = tf.train.Saver(max_to_keep=hp.num_epochs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 832, in __init__\n    self.build()\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 844, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 881, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 510, in _build_internal\n    save_tensor = self._AddSaveOps(filename_tensor, saveables)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 210, in _AddSaveOps\n    save = self.save_op(filename_tensor, saveables)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 124, in save_op\n    tensors)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py\", line 1807, in save_v2\n    name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nResourceExhaustedError (see above for traceback): log/1/iwslt2016_E09L3.15-13806.data-00000-of-00001.tempstate7212661795582182527; No space left on device\n\t [[node save/SaveV2 (defined at <ipython-input-26-a868abf1b972>:2) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[node save/SaveV2 (defined at <ipython-input-26-a868abf1b972>:2) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"# Session\")\n",
    "saver = tf.train.Saver(max_to_keep=hp.num_epochs)\n",
    "with tf.Session() as sess:\n",
    "    ckpt = tf.train.latest_checkpoint(hp.logdir)\n",
    "    if ckpt is None:\n",
    "        logging.info(\"Initializing from scratch\")\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        save_variable_specs(os.path.join(hp.logdir, \"specs\"))\n",
    "    else:\n",
    "        saver.restore(sess, ckpt)\n",
    "\n",
    "    summary_writer = tf.summary.FileWriter(hp.logdir, sess.graph)\n",
    "\n",
    "    sess.run(train_init_op)\n",
    "    total_steps = hp.num_epochs * num_train_batches\n",
    "    _gs = sess.run(global_step)\n",
    "    for i in tqdm(range(_gs, total_steps+1)):\n",
    "        _, _gs, _summary = sess.run([train_op, global_step, train_summaries])\n",
    "        epoch = math.ceil(_gs / num_train_batches)\n",
    "        summary_writer.add_summary(_summary, _gs)\n",
    "\n",
    "        if _gs and _gs % num_train_batches == 0:\n",
    "            logging.info(\"epoch {} is done\".format(epoch))\n",
    "            _loss = sess.run(loss) # train loss\n",
    "\n",
    "            logging.info(\"# test evaluation\")\n",
    "            _, _eval_summaries = sess.run([eval_init_op, eval_summaries])\n",
    "            summary_writer.add_summary(_eval_summaries, _gs)\n",
    "\n",
    "            logging.info(\"# get hypotheses\")\n",
    "            hypotheses = get_hypotheses(num_eval_batches, num_eval_samples, sess, y_hat, m.idx2token)\n",
    "\n",
    "            logging.info(\"# write results\")\n",
    "            model_output = \"iwslt2016_E%02dL%.2f\" % (epoch, _loss)\n",
    "            if not os.path.exists(hp.evaldir): os.makedirs(hp.evaldir)\n",
    "            translation = os.path.join(hp.evaldir, model_output)\n",
    "            with open(translation, 'w') as fout:\n",
    "                fout.write(\"\\n\".join(hypotheses))\n",
    "\n",
    "            logging.info(\"# calc bleu score and append it to translation\")\n",
    "            calc_bleu(hp.eval3, translation)\n",
    "\n",
    "            logging.info(\"# save models\")\n",
    "            ckpt_name = os.path.join(hp.logdir, model_output)\n",
    "            saver.save(sess, ckpt_name, global_step=_gs)\n",
    "            logging.info(\"after training of {} epochs, {} has been saved.\".format(epoch, ckpt_name))\n",
    "\n",
    "            logging.info(\"# fall back to train mode\")\n",
    "            sess.run(train_init_op)\n",
    "    summary_writer.close()\n",
    "\n",
    "\n",
    "logging.info(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
